# Perceptron을 이용한 Neural Network 구현 - 2017920051 이제우

본 보고서는 본 디렉토리에 포함된 신경망 구현에 대하여 설명한다.


## 빌드

다음과 같이 빌드한다:

    make

`build` 디렉토리에 오브젝트 파일이 생성되며, `bin` 디렉토리에 실행 파일들이 생성된다.


## 실행

빌드 후 `bin` 디렉토리에 생성된 각 실행 파일은 다음과 같은 명령줄 인자를 받는다:

* `--seed=<SEED>`: 신경망 초기화를 위한 시드값을 지정한다. 지정하지 않을 경우, 임의의 시드가 사용된다.

* `--lrate=<LEARNING_RATE>`: 신경망 학습시 사용되는 학습률을 지정한다. 기본값은 0.001이다.

* `--ethr=<ERROR_THRESHOLD>`: 오차의 한계값을 지정한다. 오차 값이 지정된 값의 아래로 내려갈
  경우, 학습이 종료된다. 기본값은 0.03이다.

* `--pintv=<PRINT INTERVAL>`: 각 출력 사이의 간격을 지정한다. 기본값은 0이다. 각 프로그램은
  Epoch 1에 초기 신경망 데이터를 출력하며, 이후 PRINT_INTERVAL Epoch 동안은 출력하지 않고, 그 다음
  Epoch에 다시 한 번 출력한다. 예외적으로, 오차 값이 지정된 경계값 아래로 내려가 학습이 종료된 마지막
  Epoch에는 값을 출력한다.

출력은 표준 출력을 통해 이루어진다.


### 예시

`xor`를 시드값을 42, 학습률을 0.01, 출력 간격을 999로 하여 실행해 출력을 `out.txt`에
저장하려 할 경우, 다음과 같이 할 수 있다:

    ./xor --seed=42 --lrate=0.01 --pintv=999 > out.txt


## 구현

이 신경망 구현은 학습을 위해 Online Stochastic gradient descent 방식을 이용하며, 풀어야 하는 문제가
Binary classification 문제임을 고려하여 Binary cross entropy 값을 최소화할 오차 값으로 사용한다.

### 구현의 한계
Online SGD 외의 최적화 방식은 별도로 구현되어있지 않으며, 오차값의 계산 및 Backward propagation에 필요한
오차값의 계산이 모듈화되어있지 않아 사용자가 그 값을 직접 계산해야 하는 반면, 오차값의 편미분 값을 계산하는
`ComputationLayer::learn` 멤버 함수는 Binary cross entropy에 대한 값을 계산하도록 고정되어있다. 이는
개선해야 할 사항이다.


## 디렉토리 구조

* **`include`** 디렉토리: 헤더 파일들. 템플릿의 사용으로 인해, 헤더 파일에 구현이 포함된 경우가 많음에 유의하라.

  * **`activations.hpp`**: 활성화 함수 `ReLU`, `Sigmoid`, `Tanh`의 선언. 활성화 함수는 함수와
    그 도함수의 쌍으로 표현되며, 세 함수 모두 별도의 상태를 가지지 않으므로 그 멤버 함수가
    `static`으로 선언되어 있다. 즉, 호출을 위해 인스턴스를 생성할 필요가 없다. 해당 함수들은
    이후 `Perceptron` 등의 템플릿 인자로 사용된다.

  * **`perceptron.hpp`**: `Perceptron` 클래스의 선언 및 구현. 여기에 구현된 퍼셉트론은
    컴파일 타임에 유저가 지정한 고정된 수의 입력을 가지며, 각 입력에 대한 weight
    값 및 bias 값을 상태로 가진다. Forward pass 및 Backward propagation이 구현되어있다.

  * **`layers.hpp`**: 신경망의 레이어 선언 및 구현. `InputLayer`는 단순히 그 입력 값을 그대로
    출력하는 레이어이며, `ComputationLayer`는 고정된 수의 `Perceptron`을 상태로 가지는 레이어이다.
    모든 레이어는 `static constexpr std::size_t size()`를 멤버 함수로 가지며, 이를 이용해
    각 `ComputationLayer`는 해당 레이어의 퍼셉트론이 몇 개의 입력을 가져야 할지 결정한다.
    Forward pass 및 Backward propagation이 구현되어 있다.

  * **`random_iterator.hpp`**: 퍼셉트론 및 레이어의 weight 초기값을 iterator 인터페이스를 통해
    전달하기 위해, `<random>`에 포함된 난수 생성기에 iterator 인터페이스를 통해 접근할 수 있게 하는
    `RandomIterator` 클래스가 선언 및 구현되어 있다.

  * **`run.hpp`**: 실질적 `main` 함수인 `run` 함수가 포함되어 있다. 해당 함수는 `src/and.cpp` 등에서
    사용되어 코드의 중복을 줄인다.

* **`src`** 디렉토리: 소스 코드 파일들.

  * **`activations.cpp`**: `activations.hpp`의 구현.

  * **`and.cpp`**, **`or.cpp`**, **`xor.cpp`**, **`donut.cpp`**: 각 경우에 해당하는 네트워크 구조 선언과
    학습 데이터를 포함한다.

* **`out`** 디렉토리: 실행 파일로 생성한 예시 출력 파일들.

* **`plot`** 디렉토리: 출력 파일의 시각화를 위한 Python 스크립트 및 예시 파일들의 시각화 결과들.
  상세한 것은 별도로 설명한다.

* **`README.md`**: 이 문서.

* **`Makefile`**: 빌드를 위한 makefile.


## 시각화

plot 디렉토리에 포함된 `run.py`를 이용해 실행파일의 출력을 시각화 할 수 있다.
`python3`, `numpy`, `matplotlib`이 설치된 환경에서, 다음과 같이 실행한다:

    python run.py <INPUT FILE> [<MARKER FILE>]

이후 현재 작업 디렉토리에 입력 파일과 이름이 같은 디렉토리가 생성되며, 해당 디렉토리에
시각화 결과가 저장된다. 마커 파일은 사용할 경우 시각화 결과에 기대 출력값을 표시하는데에 사용할 수 있다.
예시 마커 파일은 plot/markers 디렉토리에 저장되어있다.

시각화 결과는 다음을 포함한다.

* 각 Epoch에 대한 각 레이어의 출력 (`<EPOCH NUMBER>.png`): 모든 레이어가 Tanh 함수를 사용한다는 가정하에 작동한다. 시각화 결과에서
  주황색은 음수, 푸른색은 양수를 나타낸다.

* `error.png`: Epoch 경과에 따른 오차값의 변화를 나타낸다.

### 예시

`xor`의 실행 결과로 생성된 `xor_out.txt` 파일을 XOR에 해당하는 마커를 이용해 시각화 하려고 할 경우,

    python run.py xor_out.txt markers/xor.csv

이후 작업 디렉토리에 시각화 결과를 포함하고 있는 `xor_out` 디렉토리가 생성된다.


## 고찰

### Local minimum/Saddle point

`out` 디렉토리의 `xor_seed0_lrate0p3177.txt`와 `xor_seed0_lrate0p3178.txt`는 시드값 0인 경우를 각각 0.3177,
0.3178의 학습률로 학습시킨 결과에서 처음 301 Epoch까지만을 기록한 것으로, 0.3177로 학습시킨 경우는 오차값이 어느 수준 아래로
떨어지지 않는 반면 0.3178로 학습시킨 경우 빠르게 감소헤 약 300 Epoch만에 학습이 종료되는 것을 볼 수 있다.

이는 0.3177로 학습시킨 경우의 신경망이 Local minimum 또는 Saddle point에 수렴한 것으로 해석할 수 있으며, 실제로 0.3177의
학습률로 계속 학습시킨 경우 약 6,100,000 Epoch 후 오차값의 큰 변화 없이 weight 값의 변화가 멈춘다
(`xor_seed0_lrate0p3177_skip.txt`).

0.3178의 학습률을 사용한 경우 학습이 빠르게 종료되는 것은 SGD 사용시 해당 문제를 학습률을 적당히 높이는 것으로 해결할 수 있음을
보여주는 동시에, 신경망의 학습 속도가 학습률의 작은 변화에도 민감하게 변화할 수 있음을 보여준다. 즉, SGD 사용시 적절한 학습률의
결정은 매우 중요한 요소이다. 실제로는 SGD 이외의 최적화 방식이 권장되며, 이러한 여타 방식을 통해 학습률 결정 문제를 해결할 수
있을 것으로 기대할 수 있다.


### Dying ReLU

시각화의 곤란함으로 인해 예시에는 포함되어있지 않으나, ReLU 활성화 함수를 사용할 경우 일부 퍼셉트론이 '죽는' 것을 확인할 수 있었다.
즉, 해당 퍼셉트론은 모든 학습 데이터에 대해 0을 출력하며, 그 weight 값이 변화하지도 않는다. 이 경우 Leaky ReLU 등의 대체 활성화
함수를 이용해 문제를 부분적으로 해결할 수 있다.


### Donut

시드값 9로 초기화된 신경망에 도넛 형태의 데이터를 학습시킨 경우에 해당하는 출력파일은 `out/donut_seed9.txt`이며, 그 시각화 자료는
`plot/donut_seed9`에 저장되어있다. 해당 시각화 결과를 통해, 학습이 끝난 신경망의 출력이 (학습 데이터에는 부합하지만) 실제로는
도넛 모양이 아님을 볼 수 있으며, 이 경우는 학습 데이터의 추가를 통해 문제를 해결할 수 있을 것으로 기대된다.